# Template for the config file,
# Defaults to config.yml in ./config.
# The config tool defaults to config/config.config-tool.yml, and uses the same config 
# options, though only EndpointURL and if needed username/password/secure are required.
# The values here are generally the default values, except for the contents of lists etc.
# Your config file can contain only a subset of these config options. Any extra options will
# cause the extractor to fail.

# Version of the config schema
version: 1

source:
    # The URL of the OPC-UA server to connect to
    endpoint-url: "opc.tcp://localhost:4840"
    # Auto accept connections from unknown servers.
    # There is not yet a feature to accept new certificates, but you can manually move rejected certificates to accepted.
    # Paths are defined in opc.ua.net.extractor.Config.xml
    auto-accept: false
    # How often the client requests updates to subscribed variables.
    # 0 updates at maximum rate set by server
    # This decides the maximum rate points are pushed to CDF. Higher reduces network and server load.
    publishing-interval: 500
    # How often the server requests updates from the source system (the source system is often the server itself)
    # 0 uses maximum rate set by server
    # Lower increases server load. This should be set to below the maximum update frequency of the source system.
    sampling-interval: 100
    # Length of internal server queue for each subscribed item. < 2 means that any updates occuring between publish requests are lost.
    queue-length: 10
    # OPC-UA username, leave blank for anonymous user. (no authentication)
    username:
    # OPC-UA password
    password:
    # Use secured connection
    secure: false
    # If true, force restart the extractor on disconnect. Required for some servers
    force-restart: false
    # Completely exit the extractor on failure, instead of doing an internal restart. Internal reset should re-initialize everything relevant,
    # but this can be set to do restart with some external tool instead.
    exit-on-failure: false
    # Restart the extractor on reconnect. If the server handles reconnecting and is not expected to have important structural changes
    # on restart, this can be left as false for smoother reconnects (in this case, OPC-UA has systems in place to handle reconnecting).
    restart-on-reconnect: false
    # Interval in ms between each keep-alive request to the server. If the extractor times out during startup, increasing this may help.
    # Timeout is 2 * interval + 100ms, this can happen if the server is poorly parallelized and hangs during heavy load.
    keep-alive-interval: 5000
    # Max-sizes of various requests to OPC-UA. Defaults are usually okay, but some servers may fail with too large request sizes
    # Max number of results per browse or browseNext operation
    browse-chunk: 1000
    # Max number of nodes to browse at the same time. Higher is much faster, but may be restricted by the server.
    browse-nodes-chunk: 1000
    # Maximum number of attributes to request per call to the server. Higher is faster.
    attributes-chunk: 10000
    # Maximum number of monitoredItems to create on the server per call. Higher is faster.
    subscription-chunk: 1000

# Config for reading of history from the server
history:
    # Enable/disable history synchronization from the OPC-UA server to CDF.
    # This is a master switch covering both events and data
    enabled: false
    # Enable or disable data history on historizing nodes. "Enabled" must be true.
    data: true
    # Enable/disable backfill behavior. If this is false, data is read using frontfill only. (Pre 1.1 behavior)
    # This applied to both datapoints and events.
    backfill: false
    # Max number of datapoints per history read request, 0 for server specified
    data-chunk: 1000
    # Max number of simultaneous nodes per historyRead request for data
    data-nodes-chunk: 100
    # Max number of events per history read request, 0 for server specified
    event-chunk: 1000
    # Max number of simultaneous nodes per historyRead request for events
    event-nodes-chunk: 100
    # The earliest timestamp to be read from history on the OPC-UA server, in milliseconds since 1/1/1970.
    start-time: 0
    # Granularity to use when doing historyRead, in seconds. Nodes with last known timestamp within this range of eachother will
    # be read together. Should not be smaller than usual average update rate
    # Leave at 0 to always read a single node each time.
    granularity: 600


# Configuration for the pusher to CDF.
cognite:
    # The project to connect to in the API
    project:
    # Cognite api key
    api-key:
    # Cognite service url
    host: "https://api.cognitedata.com"
    # Debug mode, if true, Extractor will not push to target
    debug: false
    # Replace all instances of NaN or Infinity with this floating point number. If left empty, ignore instead.
    non-finite-replacement:
    # Whether to read start/end-points on startup, where possible. At least one pusher should be able to do this,
    # otherwise back/frontfill will run for the entire history every restart.
    # The CDF pusher is not able to read start/end points for events, so if reading historical events is enabled, one other pusher
    # able to do this should be enabled.
    # The state-store can do all this, if the state-store is enabled this can still be enabled if timeseries have been deleted from CDF
    # and need to be re-read from history.
    read-extracted-ranges: true
    # Data set to use for new objects. Existing objects will not be updated
    data-set-id:

    # Default empty. Store assets and/or timeseries data in raw. Assets will not be created at all,
    # timeseries will be created with just externalId, isStep and isString.
    # Both timeseries and assets will be persisted in their entirety to raw.
    # Datapoints are not affected, events will be created, but without asset context. The externalId
    # of the source node is added to metadata if applicable.
    # Use different table names for assets and timeseries.
    raw-metadata:
        # Database to store data in, required.
        database:
        # Table to store assets in.
        assets-table:
        # Table to store timeseries in.
        timeseries-table:

    # Map metadata to asset/timeseries attributes. Each of "assets" and "timeseries" is a map from property DisplayName to
    # CDF attribute. Legal attributes are "name, description, parentId" and "unit" for timeseries. "parentId" must somehow refer to
    # an existing asset. For timeseries it must be a mapped asset, for assets it can be any asset.
    # Example usage:
    # timeseries:
    #    "EngineeringUnits": "unit"
    #    "EURange": "description"
    # assets:
    #    "Name": "name"
    metadata-mapping:
        assets:
        timeseries:
    # Config for authentication if a bearer access token has to be used for authentication.
    # Leave empty to disable.
    idp-authentication:
        # Application Id
        client-id:
        # Directory tenant
        tenant:
        # Client secret
        secret:
        # List of resource scopes, ex:
        # scopes:
        #   - scopeA
        #   - scopeB
        scopes:
        # Which implementation to use in the authenticator. One of
        # MSAL (recommended) - Microsoft Authentication Library
        # Basic - Post to authentication endpoint and parse JSON response
        # Default is MSAL
        implementation: MSAL
        # Identity provider authority endpoint (optional)
        authority: "https://login.microsoftonline.com/"
        # Minimum time-to-live in seconds for the token (optional)
        min-ttl: 30
    # Configure automatic retries on requests to CDF. Can be left out to keep default values.
    cdf-retries:
        # Timeout in ms for each individual try
        timeout: 80000
        # Maximum number of retries, less than 0 retries forever
        max-retries: 5
        # Max delay in ms between each try. Base delay is calculated according to 125*2^retry ms.
        # If less than 0, there is no maximum. (0 means that there is never any delay)
        max-delay: 5000
    # Configure chunking of data on requests to CDF. Note that increasing these may cause requests to fail due to limits in CDF.
    cdf-chunking:
        # Maximum number of timeseries per get/create timeseries request
        time-series: 1000
        # Maximum number of assets per get/create asset request
        assets: 1000
        # Maximum number of timeseries per datapoint create request
        data-point-time-series: 10000
        # Maximum number of datapoints per datapoint create request
        data-points: 100000
        # Maximum number of timeseries per datapoint read request, used when getting the first point in a timeseries
        data-point-list: 100
        # Maximum number of timeseries per datapoint read latest request, used when getting last point in a timeseries
        data-point-latest: 100
        # Maximum number of rows per request to cdf raw. Used with raw state-store.
        raw-rows: 10000
        # Maximum number of events per get/create events request
        events: 1000
    # Configure how requests to CDF should be throttled
    cdf-throttling:
        # Maximum number of parallel requests per timeseries operation
        time-series: 20
        # Maximum number of parallel requests per assets operation
        assets: 20
        # Maximum number of parallel requests per datapoints operation
        data-points: 10
        # Maximum number of parallel requests per raw operation
        raw: 10
        # Maximum number of parallel requests per get first/last datapoint operation
        ranges: 20
        # Maximum number of parallel requests per events operation
        events: 20
    # Configure if the SDK should do logging of requests
    sdk-logging:
        # True to disable logging from the SDK
        disable: false
        # Level of logging, one of trace, debug, information, warning, error, critical, none
        level: debug
        # Format of the log message
        format: "CDF ({Message}): {HttpMethod} {Url} - {Elapsed} ms"


# Push to an influx-database. Data-variables are mapped to series with the given id.
# Events are mapped to series with ids on the form [id].[eventId] where id is given by the source node.
influx:
    # Host URI, ex localhost:8086
    host:
    # Influx username
    username:
    # Influx password
    password:
    # Database to connect to, will not be created automatically
    database:
    # Debug mode, if true, Extractor will not push to target
    debug:
    # Replace all instances of NaN or Infinity with this floating point number. If left empty, ignore instead.
    non-finite-replacement:
    # Whether to read start/end-points on startup, where possible. At least one pusher should be able to do this,
    # otherwise back/frontfill will run for the entire history every restart.
    read-extracted-ranges: true
    # Max number of points to send in each request to influx
    point-chunk-size: 100000


# Push to an MQTT broker. Requres a separate application to be running with access to CDF
# that translates MQTT messages to requests to CDF and handles missing ids etc.
# This setup allows the extractor to run in secure environments like zone 3.
mqtt:
    # TCP Broker URL
    host:
    # TCP Broker port
    port:
    # MQTT broker username
    username:
    # MQTT broker password
    password:
    # True to enable TLS
    use-tls:
    # Mqtt client id. Should be unique for a given broker.
    client-id: cognite-opcua-extractor
    # Data set to use for new objects. Existing objects will not be updated
    data-set-id:
    # Assets topic
    asset-topic: cognite/opcua/assets
    # Timeseries topic
    ts-topic: cognite/opcua/timeseries
    # Events topic
    event-topic: cognite/opcua/events
    # Datapoints topic
    datapoint-topic: cognite/opcua/datapoints
    # Raw topic
    raw-topic: cognite/opcua/raw
    # Set to enable storing a list of created assets/timeseries to local litedb.
    # Requires the StateStorage.Location property to be set.
    # If this is left empty, metadata will have to be read each time the extractor restarts.
    # Default is empty
    local-state: mqtt_create_states
    # Timestamp in ms since epoch to invalidate stored mqtt states.
    # On extractor restart, assets/timeseries created before this will be attempted re-created in CDF.
    # They will not be deleted or updated.
    invalidate-before: 0
    # If true, pusher will not push to target
    debug: false
    # Replace all instances of NaN, Infinity or values greater than 1E100 with this floating point number. If left empty, ignore instead.
    non-finite-replacement:
    # Default empty. Store assets and/or timeseries data in raw. Assets will not be created at all,
    # timeseries will be created with just externalId, isStep and isString.
    # Both timeseries and assets will be persisted in their entirety to raw.
    # Datapoints are not affected, events will be created, but without asset context. The externalId
    # of the source node is added to metadata if applicable.
    # Use different table names for assets and timeseries.
    raw-metadata:
        # Database to store data in, required.
        database:
        # Table to store assets in.
        assets-table:
        # Table to store timeseries in.
        timeseries-table:
    # Map metadata to asset/timeseries attributes. Each of "assets" and "timeseries" is a map from property DisplayName to
    # CDF attribute. Legal attributes are "name, description, parentId" and "unit" for timeseries. "parentId" must somehow refer to
    # an existing asset. For timeseries it must be a mapped asset, for assets it can be any asset.
    # Example usage:
    # timeseries:
    #    "EngineeringUnits": "unit"
    #    "EURange": "description"
    # assets:
    #    "Name": "name"
    metadata-mapping:
        assets:
        timeseries:

        

# If a pusher fails to push data for some reason, the failure buffer will automatically store the data,
# then add it back into the queue once a push succeeds.
# If all points/events are historized, this does nothing.
failure-buffer:
    # If false, buffering is disabled
    enabled: false
    # Use an influxdb pusher as buffer. Requires an influxdb pusher.
    # This is intended to be used if there is a local influxdb instance running.
    # If points are received on non-historical points while the connection to CDF is down,
    # they are read from influxdb once the connection is restored.
    influx: false
    # If state-storage is configured, this can be used to store the ranges of points buffered in influxdb, so that
    # they can be recovered even if the extractor goes down.
    influx-state-store: false
    # Store points to a binary file for datapoints. There is no safety, and a bad write can corrupt the file,
    # but it is very fast.
    # Path to a local binary buffer file for datapoints.
    datapoint-path:
    # Path to a local binary buffer file for events.
    # The two buffer file paths must be different.
    event-path:

# Periodically store state in a local database to speed up starting, by not having to read state from destinations
# This allows you to set the read-extracted-ranges option to false on the pushers without having to read all of history on startup.
# If the OPC-UA server does not support history this does nothing.
state-storage:
    # Path to .db file used by the state storage, or database in CDF raw.
    location: "buffer.db"
    # Which type of database to use. One of "None", "Raw", "LiteDb"
    database: "Raw"
    # Names of the stores to use for each type of range. Raw tables or collections in litedb.
    variable-store: variable_states
    event-store: event_states
    influx-variable-store: influx_variable_states
    influx-event-store: influx_event_states
    # Interval between each write to the buffer file, in seconds. 0 or less disables the state storage.
    interval: 10


logger:
    # Writes log events at this level to the Console. One of verbose, debug, information, warning, error, fatal.
    # If not present, or if the level is invalid, Console is not used.
    console:
        level:
    # Writes log events at this level to a file. Logs will roll over to new files daily.
    # If not present, or if the level is invalid, logging to file is disabled.
    file:
        level:
        # Path for logging output. If not present, logging to file is disabled.
        path: # "logs/log.txt"
        # Maximum number of logs files that are kept in the log folder.
        retention-limit: 31
        # Rolling interval for log files. Either "day" or "hour".
        rolling-interval: "day"
    
metrics:
    # Start a metrics server in the extractor for Prometheus scrape
    server:
        host:
        port: 0
    # Multiple Prometheus PushGateway destinations:
    push-gateways:
        - host:
          job:
          username:
          password: 

extraction:
    # Global prefix for externalId towards pushers. Should be unique to prevent name conflicts in the push destinations.
    # The externalId is: IdPrefix + NamespaceMap[nodeId.NamespaceUri] + nodeId.Identifier
    id-prefix: "gp:"
    
    # Specify prefixes on DisplayName to ignore.
    ignore-name-prefix:
        # -prefix1
    
    # Specify specific DisplayNames to ignore.
    ignore-name:
        # -name1

    # Delay in ms between each push of data points to targets
    data-push-delay: 1000
    
    # Source node in the OPC-UA server. Leave empty to use the top level Objects node.
    root-node:
        # Full name of the namespace of the root node.
        namespace-uri:
        # Id of the root node, on the form "i=123" or "s=stringid" etc.
        node-id:

    # Override mappings between OPC UA node id and externalId, allowing e.g. the RootNode to be mapped to
    # a particular asset in CDF. Applies to both assets and time series.
    # NodeMap:
    #   "externalId": { NamespaceUri: "uri", NodeId: "i=123" }
    node-map:
  
    # Map OPC-UA namespaces to prefixes in CDF. If not mapped, the full namespace URI is used.
    # Saves space compared to using the full URL. Using the ns index is not safe as the order can change on the server.
    # For example:
    # NamespaceMap:
    #   "urn:cognite:net:server": cns
    #   "urn:freeopcua:python:server": fps
    #   "http://examples.freeopcua.github.io": efg
    namespace-map:

    # Config for how OPC-UA data-types are mapped to destinations
    data-types:
        # Add custom numeric types using their nodeId. is-step indicates whether the datatype is discrete,
        # enum indicates that it is an enumeration, which may be mapped to a string if enums-as-strings is true.
        # This also overwrite default behavior, so it is possible to make Integer discrete, etc.
        # Note that the type in question needs to have a sensible numerical conversion in C#, unless it is an array type or similar, 
        # in which case each element needs a conversion
        custom-numeric-types:
        #    - node-id: 
        #          namespace-uri:
        #          node-id:
        #      is-step: false
        #      enum: false

        # List of NodeIds corresponding to DataTypes that should be ignored. Timeseries with these datatypes will not be mapped to destinations.
        ignore-data-types:
            # - NamespaceUri:
            #	NodeId:

        # Assume unknown ValueRanks without ArrayDimensions are all scalar, and create timeseries in CDF accordingly.
        # If such a variable produces an array, only the first element will be mapped to CDF
        unknown-as-scalar: false

        # Maximum size of array variables. Only arrays with the ArrayDimensions property in opc-ua specified will be used,
        # leave at 0 to only allow scalar values.
        # Note that some server implementations have issues with the ArrayDimensions property, so it is not fetched at all if MaxArraySize is 0
        # -1 indicates that there is no limit to array length, though only 1-dimensional structures will be read either way.
        max-array-size: 0
   
        # Set to true to allow fetching string variables. This means that all variables with non-numeric type is converted to string in some way.
        allow-string-variables: false

        # Map out the dataType hierarchy before starting, useful if there are custom or enum types.
        # Necessary for enum metadata and for enums-as-strings to work. If this is false, any
        # custom numeric types have to be added manually.
        auto-identify-types: false

        # If this is false and auto-identify-types is true, or there are manually added enums in custom-numeric-types,
        # enums will be mapped to numeric timeseries, and labels are added as metadata fields.
        # If this is true, labels are not mapped to metadata, and enums will be mapped to string timeseries with values
        # equal to mapped label values.
        enums-as-strings: false

        # Add a metadata property dataType which contains the id of the OPC-UA datatype.
        data-type-metadata: false

        # True to treat null nodeIds as numeric instead of string
        null-as-numeric: false


    # Time in minutes between each call to browse the OPC-UA directory, then push new nodes to destinations.
    # Note that this is a heavy operation, so this number should not be set too low.
    auto-rebrowse-period: 0
    # Enable using audit events to discover new nodes. If this is set to true, the client will expect AuditAddNodes/AuditAddReferences
    # events on the server node. These will be used to add new nodes automatically, by recursively browsing from each given ParentId.
    enable-audit-discovery:

    # Update data in destinations on rebrowse or restart.
    # Set auto-rebrowse-period to some value to do this periodically.
    # Context refers to the structure of the node graph in OPC-UA. (assetId and parentId in CDF)
    # Metadata refers to any information obtained from OPC-UA properties. (metadata in CDF)
    # Enabling anything here will increase the startup- and rebrowse-time of the extractor. Enabling metadata will increase it more.
    update:
        objects:
            name: false
            description: false
            context: false
            metadata: false
        variables:
            name: false
            description: false
            context: false
            metadata: false

    # Regex on node DisplayName for treating variables as properties.
    # Variable nodes with a name that matches this regex will be treated as metadata instead of as a timeseries
    # Nodes with property type definition will always be treated as properties
    property-name-filter:
    # Equivalent filter, but on node id if the node has a string identifier
    property-id-filter:

events:
    # Events are extracted with the following rules: By default all events are extracted.
    # Set all-events to false in order to disable extracting base OPC-UA events.
    # Specify events in the event-ids option to only extract a limited list of events.
    # Emitters are added from the node hierarchy, or from the emitter-ids option,
    # if found in both, the node hierarchy is used.

    # True to enable reading events from the server, reads from any nodes in the hierarchy
    # with a suitable EventNotifier, as well as the Server node.
    enabled: false

    # Default true, enable reading both custom events and base opc-ua events.
    all-events: true

    # True to enable reading historical events from historizing event emitters
    history: false

    # Regex filter on event type DisplayName, matches will not be extracted.
    exclude-event-filter:

    # List of BrowseName for properties to be excluded from automatic mapping to destination metadata.
    # All event properties are read, by default only "Time" and "Severity" are used from the base event.
    # Be aware that a maximum of 16 metadata entries are allowed in CDF.
    exclude-properties:
        #- Property1
        #- Property2
    # Map source browse names to other values in the destination. For CDF, internal properties may be overwritten, by default
    # "Message" is mapped to description, "SourceNode" is used for context and "EventType" is used for type. These may also be excluded or replaced by 
    # overrides in DestinationNameMap. If multiple properties are mapped to the same value, the first non-null is used.

    # If "StartTime", "EndTime" or "SubType" are specified, either directly or through the map, these are used as event properties instead of metadata.
    # StartTime and EndTime should be either DateTime, or a number corresponding to the number of milliseconds since January 1 1970.
    # If no StartTime or EndTime are specified, both are set to the "Time" property of BaseEventType.
    # "Type" may be overriden case-by-case using "NameOverrides" in Extraction configuration, or in a dynamic way here. If no "Type" is specified,
    # it is generated from Event NodeId in the same way ExternalIds are generated for normal nodes.
    destination-name-map:
        #Property1: SubType

    # Event ids to map, with full namespace-uri, and node identifier on the form "i=123" or "s=somestring"
    # Custom events must be subtypes of the BaseEventType.
    # This is used to specify which specific events should be extracted, instead of just extracting all events.
    event-ids:
        #-   namespace-uri:
        #    node-id:
    # Id of nodes to be observed as event emitters. Empty Namespace/NodeId defaults to the server node.
    # This is used to add extra emitters that are not in the extracted node hierarchy, or that does not
    # correctly specify the EventNotifier property. 
    emitter-ids:
        #-   namespace-uri:
        #    node-id:

    # Subset of the emitter-ids property. Used to make certain emitters historical.
    # Requires the events.history property to be true
    historizing-emitter-ids:
        #-   NamespaceUri:
        #    NodeId: